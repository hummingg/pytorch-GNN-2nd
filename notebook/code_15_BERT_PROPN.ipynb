{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b0dbcd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-12-31T06:42:39.974709100Z",
     "start_time": "2023-12-31T06:42:39.958648900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitProjects\\pytorch-GNN\\pytorch-GNN-2nd\\notebook\n"
     ]
    }
   ],
   "source": [
    "# code_15_BERT_PROPN.py\n",
    "# !pip install transformers\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "# GAP dataset: https://github.com/google-research-datasets/gap-coreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558d48c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-31T06:42:54.835541300Z",
     "start_time": "2023-12-31T06:42:51.942268800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#提取代词特征\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fac5d41",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BERT_PATH = '/home/xhm/nlp/pytorch-GNN/pytorch-GNN-2nd/huggingface/bert-base-uncased'\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "\n",
    "# print(tokenizer.tokenize('I have a good time, thank you.'))\n",
    "\n",
    "# bert = BertModel.from_pretrained(BERT_PATH)\n",
    "\n",
    "# print('load bert model over')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3045077",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-31T06:43:02.834320100Z",
     "start_time": "2023-12-31T06:43:02.751472700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#指定设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#读取数据    \n",
    "# 变量命名有问题？\n",
    "df_test = pd.read_csv(\"../gap-coreference/gap-development.tsv\", delimiter=\"\\t\")\n",
    "df_train_val = pd.concat([\n",
    "    pd.read_csv(\"../gap-coreference/gap-test.tsv\", delimiter=\"\\t\"),\n",
    "    pd.read_csv(\"../gap-coreference/gap-validation.tsv\", delimiter=\"\\t\")\n",
    "], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "120189a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-31T07:08:16.246434300Z",
     "start_time": "2023-12-31T07:08:16.234371500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def getmodel():\n",
    "    #加载词表文件tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('../huggingface/bert-base-uncased')\n",
    "\n",
    "    #添加特殊词\n",
    "    special_tokens_dict = {'additional_special_tokens': [\"[THISISA]\", \"[THISISB]\", \"[THISISP]\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)  #添加特殊词\n",
    "    print('from getmodel', tokenizer.additional_special_tokens, tokenizer.additional_special_tokens_ids)\n",
    "\n",
    "    model = BertModel.from_pretrained('../huggingface/bert-base-uncased')  #加载模型\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "############################\n",
    "\n",
    "\n",
    "def insert_tag(row, hasbrack=True):\n",
    "    # P: 目标代词   A: 候选名称1    B: 候选名称2\n",
    "    orgtag = [\" [THISISA] \", \" [THISISB] \", \" [THISISP] \"]\n",
    "    if hasbrack == False:\n",
    "        orgtag = [\" THISISA \", \" THISISB \", \" THISISP \"]\n",
    "\n",
    "    #按照插入的位置，从大到小排序[(383, ' THISISP '), (366, ' THISISB '), (352, ' THISISA ')]\n",
    "    to_be_inserted = sorted([\n",
    "        (row[\"A-offset\"], orgtag[0]),\n",
    "        (row[\"B-offset\"], orgtag[1]),\n",
    "        (row[\"Pronoun-offset\"], orgtag[2])], key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    text = row[\"Text\"]  #len 443\n",
    "    for offset, tag in to_be_inserted:  #先插最后的，不会影响前面\n",
    "        text = text[:offset] + tag + text[offset:]  #（插到每个代词的前面）\n",
    "    return text  #len 470 (443+3*9)\n",
    "\n",
    "\n",
    "# 将标签分离，并返特殊标签偏移位置\n",
    "def tokenize(sequence_ind, tokenizer, sequence_mask=None):  #将标签分离，并返回标签偏移位置\n",
    "    entries = {}\n",
    "    final_tokens = []\n",
    "    final_mask = []\n",
    "\n",
    "    for i, one in enumerate(sequence_ind):\n",
    "        if one in tokenizer.additional_special_tokens_ids:\n",
    "            tokenstr = tokenizer.convert_ids_to_tokens(one)\n",
    "            entries[tokenstr] = len(final_tokens)\n",
    "            continue\n",
    "        final_tokens.append(one)\n",
    "        if sequence_mask is not None:\n",
    "            final_mask.append(sequence_mask[i])\n",
    "    return final_tokens, (entries[\"[THISISA]\"], entries[\"[THISISB]\"], entries[\"[THISISP]\"]), final_mask\n",
    "\n",
    "\n",
    "def savepkl(df, name):\n",
    "    bert_prediction = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        #循环内部\n",
    "        text = insert_tag(row)  #插入标签\n",
    "        sequence_ind = tokenizer.encode(text)  #向量化\n",
    "        tokens, offsets, _ = tokenize(sequence_ind, tokenizer)  #获取标签偏移\n",
    "        '''\n",
    "        [CLS ... SEP]\n",
    "        [101, 11199, 10093, 3877, 1011, 1011, 2209, 1996, 2610, 2961, 6513, 1997, 4079, 1010, 8538, 1012, 14019, 2011, 4079, 1999, 1996, 2345, 2792, 1997, 2186, 1015, 1010, 2044, 2002, 7771, 2007, 8437, 1010, 1998, 2003, 2025, 2464, 2153, 1012, 18188, 2726, 2209, 19431, 13737, 1010, 15595, 1005, 1055, 2767, 1998, 2036, 1037, 2095, 2340, 11136, 1999, 4079, 1005, 1055, 2465, 1012, 14019, 2014, 6898, 2206, 4079, 1005, 1055, 6040, 2044, 2002, 2876, 1005, 1056, 2031, 3348, 2007, 2014, 2021, 2101, 11323, 2023, 2001, 2349, 2000, 2032, 9105, 26076, 2125, 2014, 2767, 15595, 1012, 102]\n",
    "        (42, 45, 62)\n",
    "        '''\n",
    "        print(tokens, offsets)\n",
    "        token_tensor = torch.LongTensor([tokens]).to(device)\n",
    "        #         bert_outputs,bert_last_outputs=  model(token_tensor)  #[1, 107, 768] , [1, 768]\n",
    "        #         print(type(model))\n",
    "        #         <class 'str'> last_hidden_state\n",
    "        #         print(type(bert_outputs), bert_outputs)\n",
    "        #         print(type(bert_last_outputs), bert_last_outputs)\n",
    "        res = model(token_tensor)\n",
    "        # odict_keys(['last_hidden_state', 'pooler_output'])\n",
    "        # print(res.keys())\n",
    "        bert_last_hidden_state, bert_pooler_output = res[:2]  #[1, 107, 768] , [1, 768]\n",
    "        # print(bert_last_hidden_state.shape, bert_pooler_output.shape)\n",
    "        extracted_outputs = bert_last_hidden_state[:, offsets, :]  #根据偏移位置抽取单词的特征向量\n",
    "        bert_prediction.append(extracted_outputs.cpu().numpy())\n",
    "    pickle.dump(bert_prediction, open(name, \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "在使用 BERT 模型时，你可以根据任务需求选择使用 'last_hidden_state' 或 'pooler_output'，或者结合两者进行下游任务的建模。\n",
    "例如，在一些任务中，可以使用 'pooler_output' 作为整个句子的表示，而在其他任务中，可以使用 'last_hidden_state' 来获取每个单词的表示。\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "b6d8028ccd2c0b73"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e53f5417",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-31T07:08:22.405263400Z",
     "start_time": "2023-12-31T07:08:19.446587200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from getmodel ['[THISISA]', '[THISISB]', '[THISISP]'] [30522, 30523, 30524]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../huggingface/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 1/2000 [00:00<05:07,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 11199, 10093, 3877, 1011, 1011, 2209, 1996, 2610, 2961, 6513, 1997, 4079, 1010, 8538, 1012, 14019, 2011, 4079, 1999, 1996, 2345, 2792, 1997, 2186, 1015, 1010, 2044, 2002, 7771, 2007, 8437, 1010, 1998, 2003, 2025, 2464, 2153, 1012, 18188, 2726, 2209, 19431, 13737, 1010, 15595, 1005, 1055, 2767, 1998, 2036, 1037, 2095, 2340, 11136, 1999, 4079, 1005, 1055, 2465, 1012, 14019, 2014, 6898, 2206, 4079, 1005, 1055, 6040, 2044, 2002, 2876, 1005, 1056, 2031, 3348, 2007, 2014, 2021, 2101, 11323, 2023, 2001, 2349, 2000, 2032, 9105, 26076, 2125, 2014, 2767, 15595, 1012, 102] (42, 45, 62)\n",
      "[101, 2002, 3473, 2039, 1999, 6473, 2669, 1010, 4307, 1996, 2117, 4587, 1997, 2274, 2336, 2164, 2010, 3428, 1010, 5965, 1998, 5146, 1998, 5208, 1010, 25532, 1006, 27233, 7685, 1007, 1998, 14749, 1012, 2010, 2152, 2082, 2420, 2020, 2985, 2012, 2047, 25487, 2152, 2082, 1999, 2663, 7159, 2912, 1010, 4307, 1012, 11407, 3273, 2007, 6795, 24520, 2013, 4085, 2000, 3999, 1012, 2010, 3722, 1010, 5217, 1011, 6908, 8360, 11378, 2003, 4600, 5105, 2011, 1996, 11481, 12465, 1997, 26822, 4478, 10654, 8447, 1998, 22827, 4478, 3217, 10556, 21547, 1012, 102] (51, 54, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2000 [00:00<05:34,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2002, 2018, 2042, 20847, 2000, 3519, 1010, 2021, 5295, 1999, 2901, 2000, 5138, 1037, 2695, 2004, 6059, 2000, 4380, 1012, 2139, 2474, 2061, 2696, 2153, 2743, 2005, 3099, 1997, 1039, 1008, 16428, 16429, 2050, 1999, 2889, 1012, 3249, 2011, 3099, 12262, 2480, 2011, 2058, 2321, 1003, 1010, 2023, 3732, 2275, 5963, 2001, 3278, 2138, 2009, 3465, 2139, 2474, 2061, 2696, 2172, 1997, 2010, 2490, 2306, 1996, 2074, 24108, 9863, 2283, 1006, 2029, 2001, 13862, 2007, 3377, 1999, 1996, 2889, 3054, 1011, 3408, 1007, 1010, 2877, 2000, 2343, 5828, 2273, 6633, 1005, 1055, 20380, 1997, 1037, 3584, 2283, 2862, 1999, 1039, 1008, 16428, 16429, 2050, 2005, 1996, 2857, 3054, 1011, 2744, 3864, 1010, 1998, 2000, 2139, 2474, 2061, 2696, 1005, 1055, 4945, 2000, 12452, 1037, 2835, 1999, 3519, 1012, 102] (41, 57, 63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2000 [00:00<05:23,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1996, 2783, 2372, 1997, 4126, 2031, 2036, 2864, 1999, 2624, 3799, 2104, 1996, 2316, 2171, 1005, 1005, 6556, 7193, 1036, 1036, 1012, 4894, 2038, 2405, 2048, 2573, 1997, 4349, 1999, 3522, 2086, 1024, 8831, 1997, 3109, 1010, 2029, 2003, 3205, 1999, 1996, 2600, 1998, 4897, 2534, 1997, 4476, 3075, 1010, 1998, 1037, 5189, 20364, 2614, 2234, 2013, 2682, 1012, 4635, 2038, 2550, 3365, 3152, 1006, 2104, 2010, 2613, 2171, 1010, 2888, 29062, 1007, 2164, 1996, 2718, 1996, 6548, 1998, 3817, 10773, 1012, 102] (36, 71, 67)\n",
      "[101, 2014, 4203, 10768, 3850, 2834, 1999, 2384, 2001, 2004, 27617, 2401, 1999, 1996, 8001, 3179, 1997, 2175, 3669, 5558, 2615, 1005, 1055, 7110, 8447, 7849, 1012, 2016, 6369, 2006, 1996, 4745, 11605, 13250, 5302, 20846, 3405, 1997, 1996, 3850, 1012, 2005, 2010, 3850, 3460, 9593, 1010, 5922, 2128, 13088, 12184, 1996, 2535, 1997, 14433, 6728, 11837, 18826, 1010, 2761, 1037, 2033, 12036, 1011, 10430, 2535, 1010, 2005, 10430, 2376, 1010, 1998, 14043, 6369, 1996, 2128, 15773, 2112, 1997, 14433, 6728, 11837, 18826, 2012, 13677, 3850, 1997, 3190, 1010, 2139, 12311, 22492, 3366, 3850, 1010, 1998, 1996, 4956, 3850, 1012, 1010, 2035, 1999, 2289, 1012, 2016, 2038, 2144, 7042, 2195, 3033, 1998, 4395, 1999, 2198, 5922, 1005, 2573, 1010, 2164, 1996, 10430, 2112, 1999, 3449, 9152, 1008, 1051, 1010, 1998, 1996, 2535, 1997, 13970, 12274, 17516, 1999, 1037, 10902, 3392, 1999, 1996, 2848, 5271, 11650, 2537, 2012, 1996, 2047, 10249, 3246, 2782, 1999, 6004, 1012, 102] (54, 72, 105)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2000 [00:00<07:22,  4.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_36612\\3779040271.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_grad_enabled\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[0msavepkl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'output/test_bert_outputs_forPROPN.pkl'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m     \u001B[0msavepkl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_train_val\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'output/bert_outputs_forPROPN.pkl'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_36612\\1370756307.py\u001B[0m in \u001B[0;36msavepkl\u001B[1;34m(df, name)\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[1;31m#         print(type(bert_outputs), bert_outputs)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m         \u001B[1;31m#         print(type(bert_last_outputs), bert_last_outputs)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 66\u001B[1;33m         \u001B[0mres\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     67\u001B[0m         \u001B[1;31m# odict_keys(['last_hidden_state', 'pooler_output'])\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[1;31m# print(res.keys())\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1028\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1029\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1030\u001B[1;33m             \u001B[0mreturn_dict\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1031\u001B[0m         )\n\u001B[0;32m   1032\u001B[0m         \u001B[0msequence_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mencoder_outputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    615\u001B[0m                     \u001B[0mencoder_attention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    616\u001B[0m                     \u001B[0mpast_key_value\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 617\u001B[1;33m                     \u001B[0moutput_attentions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    618\u001B[0m                 )\n\u001B[0;32m    619\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    537\u001B[0m         layer_output = apply_chunking_to_forward(\n\u001B[1;32m--> 538\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeed_forward_chunk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchunk_size_feed_forward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseq_len_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    539\u001B[0m         )\n\u001B[0;32m    540\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlayer_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\transformers\\pytorch_utils.py\u001B[0m in \u001B[0;36mapply_chunking_to_forward\u001B[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[0;32m    235\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput_chunks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mchunk_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    236\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 237\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mforward_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput_tensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    238\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    239\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mfeed_forward_chunk\u001B[1;34m(self, attention_output)\u001B[0m\n\u001B[0;32m    548\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfeed_forward_chunk\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m         \u001B[0mintermediate_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 550\u001B[1;33m         \u001B[0mlayer_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mintermediate_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    551\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlayer_output\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    552\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, input_tensor)\u001B[0m\n\u001B[0;32m    460\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    461\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_tensor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 462\u001B[1;33m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    463\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    464\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLayerNorm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0minput_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 114\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    115\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenizer, model = getmodel()\n",
    "    model.to(device)\n",
    "    # 在这个代码块内关闭梯度计算，从而节省计算资源和内存。\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    # 测试集的P、A、B这些专有名词的特征向量\n",
    "    savepkl(df_test, 'output/test_bert_outputs_forPROPN.pkl')\n",
    "    savepkl(df_train_val, 'output/bert_outputs_forPROPN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf32ff4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
