{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(r'D:\\GitProjects\\pytorch-GNN\\pytorch-GNN-2nd')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Apr 11 08:03:20 2020\n",
    "@author: 代码医生工作室\n",
    "@公众号：xiangyuejiqiren   （内有更多优秀文章及学习资料）\n",
    "@来源: <PyTorch深度学习和图神经网络(卷2）——开发应用>配套代码\n",
    "@配套代码技术支持：bbs.aianaconda.com\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import dgl\n",
    "from dgl.nn.pytorch.conv import RelGraphConv\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 如何优雅import\n",
    "from code_15_BERT_PROPN import (device, df_test, df_train_val, getmodel)\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "from sklearn import metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-31T09:01:08.756144900Z",
     "start_time": "2023-12-31T09:01:02.446824100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "base_dir = 'output/'\n",
    "\n",
    "'''加载预处理文件'''\n",
    "\n",
    "# 其他词 NoPUNC：no punctuation mark，即去掉标点符号\n",
    "# 在训练数据集中，目标代词、名称A、名称B的偏移位置？\n",
    "offsets_NoPUNC = pickle.load(open(base_dir + 'offsets_NoPUNC.pkl', \"rb\"))\n",
    "# 在训练数据集中，文本的子词向量序列\n",
    "tokens_NoPUNC = pickle.load(open(base_dir + 'tokens_NoPUNC_padding.pkl', \"rb\"))  # tokens of every sentence without padding\n",
    "# # 在训练数据集中，BERT模型的输出结果\n",
    "bert_forNoPUNC = pickle.load(open(base_dir + 'bert_outputs_forNoPUNC.pkl', \"rb\"))  # list of outputs of bert for every sentence\n",
    "# 在训练数据集中，BERT last_hidden_state 中对应目标代词、名称A、名称B的那3个向量\n",
    "PROPN_bert = pickle.load(open(base_dir + 'bert_outputs_forPROPN.pkl', \"rb\"))\n",
    "\n",
    "# offsets_NoPUNC    2454,3\n",
    "# tokens_NoPUNC     2454,266\n",
    "# bert_forNoPUNC    2454,1,266,768\n",
    "# PROPN_bert        2454,1,3,768"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-31T09:01:43.408935800Z",
     "start_time": "2023-12-31T09:01:21.911055Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[THISISA]', '[THISISB]', '[THISISP]'] [30522, 30523, 30524]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huggingface/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer, _ = getmodel()  # 加载BERT分词工具\n",
    "# parser = spacy.load('en')  # 加载SpaCy模型  'en_core_web_sm')#en_core_web_lg\n",
    "parser = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-31T09:03:28.024224600Z",
     "start_time": "2023-12-31T09:03:23.614861800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 72, 75)\n",
      "[101, 2588, 2037, 9920, 2046, 1996, 12849, 26730, 15758, 3873, 2223, 2139, 2187, 6435, 2000, 3696, 1037, 3206, 1999, 2762, 2007, 15501, 2278, 1049, 13316, 10222, 1997, 1996, 3972, 2006, 2238, 2044, 11847, 1996, 2446, 2528, 2007, 1996, 1049, 13316, 10222, 2136, 1999, 2002, 2187, 1996, 2252, 1998, 2001, 3856, 2039, 2011, 3507, 3972, 2217, 15501, 2278, 4702, 9695, 1999, 2251, 2280, 7097, 2121, 5639, 10514, 3334, 1998, 4386, 12968, 3960, 2024, 2139, 1055, 27328, 2010, 5542, 2003, 5135, 3748, 1055, 6585, 2952, 4575, 10514, 3334, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "70\n",
      "3960\n",
      "bob de his \n",
      "alonso alfredo him \n",
      "ali saddam he \n"
     ]
    }
   ],
   "source": [
    "# (70, 72, 75)\n",
    "print(offsets_NoPUNC[0])\n",
    "# [101, ..., 102, 0 * m]\n",
    "print(tokens_NoPUNC[0])\n",
    "print(offsets_NoPUNC[0][0])\n",
    "print(tokens_NoPUNC[0][70])\n",
    "# A: bob, B: de, P: his.\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print(tokenizer.convert_ids_to_tokens(tokens_NoPUNC[i][offsets_NoPUNC[i][j]]), end=' ')\n",
    "    print()\n",
    "\n",
    "# print(bert_forNoPUNC[0])\n",
    "# print(PROPN_bert[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T09:09:41.570527Z",
     "start_time": "2023-12-31T09:09:41.503399100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Name (Local): x\n",
      "Variable Name (Local): y\n",
      "Variable Name (Local): z\n",
      "Variable Name (Local): offsets_NoPUNC\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "def print_variable_name(variable):\n",
    "    # 获取当前作用域的局部变量和全局变量\n",
    "    frame = inspect.currentframe().f_back\n",
    "    local_vars = frame.f_locals\n",
    "    global_vars = frame.f_globals\n",
    "\n",
    "    # 搜索变量并打印名称\n",
    "    for name, value in local_vars.items():\n",
    "        if value is variable:\n",
    "            print(f\"Variable Name (Local): {name}\")\n",
    "            return\n",
    "\n",
    "    for name, value in global_vars.items():\n",
    "        if value is variable:\n",
    "            print(f\"Variable Name (Global): {name}\")\n",
    "            return\n",
    "\n",
    "    print(\"Variable Name: Not found\")\n",
    "\n",
    "# 示例变量\n",
    "x = 42\n",
    "y = [1, 2, 3]\n",
    "z = \"Hello\"\n",
    "\n",
    "# 查看变量名称\n",
    "print_variable_name(x)\n",
    "print_variable_name(y)\n",
    "print_variable_name(z)\n",
    "print_variable_name(offsets_NoPUNC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([2, 3])\n",
      "Data Type: torch.int64\n",
      "Index: 0\n",
      "  Type: <class 'torch.Tensor'>\n",
      "  Shape: torch.Size([3])\n",
      "  Data Type: torch.int64\n",
      "  Index: 0\n",
      "    Type: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([])\n",
      "    Data Type: torch.int64\n",
      "int\n",
      "Type: <class 'int'>\n",
      "dict\n",
      "Type: <class 'dict'>\n",
      "Keys: ['a']\n",
      "Key: a\n",
      "  Type: <class 'str'>\n",
      "offsets_NoPUNC\n",
      "Type: <class 'list'>\n",
      "Shape: 2454\n",
      "Index: 0\n",
      "  Type: <class 'tuple'>\n",
      "  Shape: 3\n",
      "  Index: 0\n",
      "    Type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "def print_variable_structure(variable, indent=\"\"):\n",
    "    print(f\"{}\\n\")\n",
    "    print(f\"{indent}Type: {type(variable)}\")\n",
    "    if isinstance(variable, dict):\n",
    "        print(f\"{indent}Keys: {list(variable.keys())}\")\n",
    "        for key, value in variable.items():\n",
    "            print(f\"{indent}Key: {key}\")\n",
    "            print_variable_structure(value, indent + \"  \")\n",
    "            break\n",
    "    elif isinstance(variable, (list, tuple, np.ndarray, torch.Tensor)):\n",
    "        if isinstance(variable, (list, tuple)):\n",
    "            print(f\"{indent}Shape: {len(variable)}\")\n",
    "            if len(variable) == 0:\n",
    "                return\n",
    "        if isinstance(variable, (np.ndarray, torch.Tensor)):\n",
    "            print(f\"{indent}Shape: {variable.shape}\")\n",
    "            print(f\"{indent}Data Type: {variable.dtype}\")\n",
    "            if isinstance(variable, (torch.Tensor)) and variable.dim() == 0 or \\\n",
    "                isinstance(variable, (np.ndarray)) and variable.ndim == 0 :\n",
    "                return\n",
    "        for index, value in enumerate(variable):\n",
    "            print(f\"{indent}Index: {index}\")\n",
    "            print_variable_structure(value, indent + \"  \")\n",
    "            break\n",
    "    elif hasattr(variable, \"__dict__\"):\n",
    "        print(f\"{indent}Attributes: {list(variable.__dict__.keys())}\")\n",
    "        for attr_name, attr_value in variable.__dict__.items():\n",
    "            print(f\"{indent}Attribute: {attr_name}\")\n",
    "            print_variable_structure(attr_value, indent + \"  \")\n",
    "            break\n",
    "\n",
    "# 示例变量\n",
    "print('tensor')\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print_variable_structure(x)\n",
    "print('int')\n",
    "x = 1\n",
    "print_variable_structure(x)\n",
    "print('dict')\n",
    "print_variable_structure({'a': 'aa'})\n",
    "print('offsets_NoPUNC')\n",
    "print_variable_structure(offsets_NoPUNC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offsets_NoPUNC\n",
      "Type: <class 'list'>\n",
      "Shape: 2454\n",
      "Index: 0\n",
      "  Type: <class 'tuple'>\n",
      "  Shape: 3\n",
      "  Index: 0\n",
      "    Type: <class 'int'>\n",
      "tokens_NoPUNC\n",
      "Type: <class 'list'>\n",
      "Shape: 2454\n",
      "Index: 0\n",
      "  Type: <class 'list'>\n",
      "  Shape: 266\n",
      "  Index: 0\n",
      "    Type: <class 'int'>\n",
      "bert_forNoPUNC\n",
      "Type: <class 'list'>\n",
      "Shape: 2454\n",
      "Index: 0\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (1, 266, 768)\n",
      "  Data Type: float32\n",
      "  Index: 0\n",
      "    Type: <class 'numpy.ndarray'>\n",
      "    Shape: (266, 768)\n",
      "    Data Type: float32\n",
      "    Index: 0\n",
      "      Type: <class 'numpy.ndarray'>\n",
      "      Shape: (768,)\n",
      "      Data Type: float32\n",
      "      Index: 0\n",
      "        Type: <class 'numpy.float32'>\n",
      "PROPN_bert\n",
      "Type: <class 'list'>\n",
      "Shape: 2454\n",
      "Index: 0\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (1, 3, 768)\n",
      "  Data Type: float32\n",
      "  Index: 0\n",
      "    Type: <class 'numpy.ndarray'>\n",
      "    Shape: (3, 768)\n",
      "    Data Type: float32\n",
      "    Index: 0\n",
      "      Type: <class 'numpy.ndarray'>\n",
      "      Shape: (768,)\n",
      "      Data Type: float32\n",
      "      Index: 0\n",
      "        Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# train: 2454\n",
    "print('offsets_NoPUNC')\n",
    "print_variable_structure(offsets_NoPUNC)\n",
    "print('tokens_NoPUNC')\n",
    "print_variable_structure(tokens_NoPUNC)\n",
    "print('bert_forNoPUNC')\n",
    "print_variable_structure(bert_forNoPUNC)\n",
    "print('PROPN_bert')\n",
    "print_variable_structure(PROPN_bert)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_offsets_NoPUNC = pickle.load(open(base_dir + 'test_offsets_NoPUNC.pkl', \"rb\"))\n",
    "test_tokens_NoPUNC = pickle.load(\n",
    "    open(base_dir + 'test_tokens_NoPUNC_padding.pkl', \"rb\"))  # tokens of every sentence without padding\n",
    "test_bert_forNoPUNC = pickle.load(\n",
    "    open(base_dir + 'test_bert_outputs_forNoPUNC.pkl', \"rb\"))  # list of outputs of bert for every sentence\n",
    "\n",
    "# 目标代词级名称\n",
    "test_PROPN_bert = pickle.load(open(base_dir + 'test_bert_outputs_forPROPN.pkl', \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_offsets_NoPUNC\n",
      "Type: <class 'list'>\n",
      "Shape: 2000\n",
      "Index: 0\n",
      "  Type: <class 'tuple'>\n",
      "  Shape: 3\n",
      "  Index: 0\n",
      "    Type: <class 'int'>\n",
      "test_tokens_NoPUNC\n",
      "Type: <class 'list'>\n",
      "Shape: 2000\n",
      "Index: 0\n",
      "  Type: <class 'list'>\n",
      "  Shape: 266\n",
      "  Index: 0\n",
      "    Type: <class 'int'>\n",
      "test_bert_forNoPUNC\n",
      "Type: <class 'list'>\n",
      "Shape: 2000\n",
      "Index: 0\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (1, 266, 768)\n",
      "  Data Type: float32\n",
      "  Index: 0\n",
      "    Type: <class 'numpy.ndarray'>\n",
      "    Shape: (266, 768)\n",
      "    Data Type: float32\n",
      "    Index: 0\n",
      "      Type: <class 'numpy.ndarray'>\n",
      "      Shape: (768,)\n",
      "      Data Type: float32\n",
      "      Index: 0\n",
      "        Type: <class 'numpy.float32'>\n",
      "test_PROPN_bert\n",
      "Type: <class 'list'>\n",
      "Shape: 2000\n",
      "Index: 0\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (1, 3, 768)\n",
      "  Data Type: float32\n",
      "  Index: 0\n",
      "    Type: <class 'numpy.ndarray'>\n",
      "    Shape: (3, 768)\n",
      "    Data Type: float32\n",
      "    Index: 0\n",
      "      Type: <class 'numpy.ndarray'>\n",
      "      Shape: (768,)\n",
      "      Data Type: float32\n",
      "      Index: 0\n",
      "        Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# test: 2000\n",
    "print('test_offsets_NoPUNC')\n",
    "print_variable_structure(test_offsets_NoPUNC)\n",
    "print('test_tokens_NoPUNC')\n",
    "print_variable_structure(test_tokens_NoPUNC)\n",
    "print('test_bert_forNoPUNC')\n",
    "print_variable_structure(test_bert_forNoPUNC)\n",
    "print('test_PROPN_bert')\n",
    "print_variable_structure(test_PROPN_bert)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 14477, 20961, 3468, 1012, 14477, 20961, 3468, 1012, 102]\n",
      "['[CLS]', 'una', '##ffa', '##ble', '.', 'una', '##ffa', '##ble', '.', '[SEP]']\n",
      "[101, 14477, 20961, 3468, 1012, 14477, 20961, 3468, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "## TEST\n",
    "# WordpieceTokenizer（以下简称 WPT）是在 BT 结果的基础上进行再一次切分，得到子词（subword，以 ## 开头），词汇表就是在此时引入的。\n",
    "# greedy longest-match-first algorithm，贪婪最长优先匹配算法。\n",
    "# unaffable -分词-> [una, ##ffa, ##ble]\n",
    "print(tokenizer.encode('unaffable.unaffable.'))\n",
    "# [101, 14477, 20961, 3468, 1012, 14477, 20961, 3468, 1012, 102]\n",
    "print(tokenizer.convert_ids_to_tokens([101, 14477, 20961, 3468, 1012, 14477, 20961, 3468, 1012, 102]))\n",
    "# ['[CLS]', 'una', '##ffa', '##ble', '.', 'una', '##ffa', '##ble', '.', '[SEP]']\n",
    "print(tokenizer.convert_tokens_to_ids(['[CLS]', 'una', '##ffa', '##ble', '.', 'una', '##ffa', '##ble', '.', '[SEP]']))\n",
    "# [101, 14477, 20961, 3468, 1012, 14477, 20961, 3468, 1012, 102]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 生成图结构数据\n",
    "def getGraphsData(tokens_NoPUNC, offsets_NoPUNC, PROPN_bert, bert_forNoPUNC):\n",
    "    '''\n",
    "    \n",
    "    Args:\n",
    "        tokens_NoPUNC:  2454,266\n",
    "        offsets_NoPUNC: 2454,3\n",
    "        PROPN_bert:     2454,1,3,768\n",
    "        bert_forNoPUNC: 2454,1,266,768\n",
    "\n",
    "    Returns:\n",
    "        all_graphs: 包含A B P及与它们直接相连的单词的子图\n",
    "        gcn_offsets: A B P在子图中的单词节点对应的索引\n",
    "    '''\n",
    "    \n",
    "    # 2454\n",
    "    assert len(tokens_NoPUNC) == len(offsets_NoPUNC) == len(PROPN_bert) == len(bert_forNoPUNC)\n",
    "    # 266=269-3. cod_16: max_len = 269  # 设置处理文本的最大长度\n",
    "    assert len(tokens_NoPUNC[0]) == len(bert_forNoPUNC[0][0])\n",
    "    assert len(offsets_NoPUNC[0]) == len(PROPN_bert[0][0]) == 3\n",
    "    # 101: [CLS]  102: [SEP]  103: [MASK]\n",
    "\n",
    "    all_graphs = []\n",
    "    gcn_offsets = []\n",
    "    for i, sent_token in enumerate(tokens_NoPUNC):\n",
    "\n",
    "        SEPid = sent_token.index(tokenizer.convert_tokens_to_ids('[SEP]'))\n",
    "\n",
    "        # 去掉所有#。不能用tokenizer.decode()，因为它会将字词合并导致offset错误\n",
    "        sent = ' '.join([re.sub(\"[#]\", \"\", token) for token in tokenizer.convert_ids_to_tokens(sent_token[1:SEPid])])\n",
    "\n",
    "        doc = parser(sent)  # 将句子切分成单词，英文中一般使用空格分隔\n",
    "        parse_rst = doc.to_json()  # 获得句子中各个单词间的依存关系树\n",
    "\n",
    "        target_offset_list = [item - 1 for item in offsets_NoPUNC[i]]  # 所有的偏移都去掉一个（[CLS]）\n",
    "\n",
    "        nodes = collections.OrderedDict()  # 带有顺序的字典 key为句子中的id，value为节点的真实索引\n",
    "        edges = []\n",
    "        edge_type = []\n",
    "\n",
    "        #  通过  parse_rst['tokens'][69]可以看到详细信息\n",
    "        # 解析依存关系\n",
    "        for i_word, word in enumerate(parse_rst['tokens']):\n",
    "            # 生成的图中，找到代词节点以及对应的边\n",
    "            if (i_word in target_offset_list) or (word['head'] in target_offset_list):\n",
    "                if i_word not in nodes:\n",
    "                    nodes[i_word] = len(nodes)  # 添加依存关系节点\n",
    "                    edges.append([i_word, i_word])  # 为节点添加自环\n",
    "                    edge_type.append(0)  # 自环关系的索引为0\n",
    "                if word['head'] not in nodes:\n",
    "                    nodes[word['head']] = len(nodes)  # 添加依存关系节点\n",
    "                    edges.append([word['head'], word['head']])  # 为节点添加自环\n",
    "                    edge_type.append(0)\n",
    "\n",
    "                # 依存关系\n",
    "                if word['dep'] != 'ROOT':\n",
    "                    edges.append([word['head'], word['id']])  # 添加依存关系边（head_id -> word_id）\n",
    "                    edge_type.append(1)  # 依存关系的索引为1\n",
    "                    edges.append([word['id'], word['head']])  # 添加反向依存关系边（head_id <- word_id）\n",
    "                    edge_type.append(2)  # 反向依存关系的索引为2\n",
    "\n",
    "        # word_id -> node_id，两种id并不一致\n",
    "        tran_edges = []\n",
    "        for e1, e2 in edges:  # 将句子中的边，换成节点间的边\n",
    "            tran_edges.append([nodes[e1], nodes[e2]])\n",
    "        # 将句子中的代词位置，换成节点中的代词索引\n",
    "        gcn_offset = [nodes[offset] for offset in target_offset_list]\n",
    "        gcn_offsets.append(gcn_offset)  # 将代词、名称A、名称B对应图中节点的索引保存起来\n",
    "\n",
    "        # 生成DGL图数据\n",
    "        G = dgl.DGLGraph()\n",
    "        G.add_nodes(len(nodes))  # 生成DGL节点\n",
    "        G.add_edges(list(zip(*tran_edges))[0], list(zip(*tran_edges))[1])\n",
    "        # 给每个节点添加特征属性\n",
    "        for i_word, word in nodes.items():\n",
    "            if (i_word in target_offset_list):  # 从PROPN_bert中获取代词、名称A、名称B的特征\n",
    "                G.nodes[[nodes[i_word]]].data['h'] = torch.from_numpy(\n",
    "                    PROPN_bert[i][0][target_offset_list.index(i_word)]).unsqueeze(0).to(device)\n",
    "            else:  # bert_forNoPUNC中获取其它词的特征\n",
    "                # +1 是因为 [CLS] 特殊标记\n",
    "                G.nodes[[nodes[i_word]]].data['h'] = torch.from_numpy(\n",
    "                    bert_forNoPUNC[i][0][i_word + 1]).unsqueeze(0).to(device)\n",
    "\n",
    "        edge_norm = []  # 边归一化算子（计算均值时的分母）\n",
    "        # e1 -> e2\n",
    "        for e1, e2 in tran_edges:\n",
    "            if e1 == e2:\n",
    "                edge_norm.append(1)  # 如果是自环边，则归一化算子为1。自环边的权重占1/(1+1)？\n",
    "            else:  # 如果是非自环边，则归一化算子为1除以去掉自环的度\n",
    "                edge_norm.append(1 / (G.in_degrees(e2) - 1))  # 去掉自环的度\n",
    "\n",
    "        # 将类型转为张量\n",
    "        edge_type = torch.from_numpy(np.array(edge_type)).type(torch.long)  # uint8 会导致错误\n",
    "        edge_norm = torch.from_numpy(np.array(edge_norm)).unsqueeze(1).float().to(device)\n",
    "\n",
    "        G.edata.update({'rel_type': edge_type, })  # 更新边类型\n",
    "        G.edata.update({'norm': edge_norm}) # 边归一化算子\n",
    "        all_graphs.append(G)  # 保存子图\n",
    "\n",
    "    return all_graphs, gcn_offsets\n",
    "\n",
    "\n",
    "def getLabelData(df):  # 生成标签\n",
    "    tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n",
    "    tmp[\"Neither\"] = ~(df[\"A-coref\"] | df[\"B-coref\"])  # 添加一个列（A和B都不指代的情况）\n",
    "    # 0: A-coref, 1: B-coref, 2: Neither\n",
    "    y = tmp.values.astype(\"bool\").argmax(1)  # 变成one-hot索引\n",
    "    return y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "# 构建数据集\n",
    "class GPRDataset(Dataset):\n",
    "    def __init__(self, y, graphs, bert_offsets, gcn_offsets, bert_embeddings):\n",
    "        self.y = y\n",
    "        self.graphs = graphs\n",
    "        self.bert_offsets = bert_offsets  # 已经+1了\n",
    "        self.bert_embeddings = bert_embeddings  # 有[CLS]\n",
    "        self.gcn_offsets = gcn_offsets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.graphs[idx], self.bert_offsets[idx], self.gcn_offsets[idx],\n",
    "                self.bert_embeddings[idx], self.y[idx])\n",
    "\n",
    "\n",
    "def collate(samples):  # 对 GPRDataset.__getitem__ 得到的批次数据重新加工\n",
    "    #    print(len(samples))#数组。个数是4（batch_size），\n",
    "\n",
    "    # 行列转换变成list\n",
    "    graphs, bert_offsets, gcn_offsets, bert_embeddings, labels = map(list, zip(*samples))\n",
    "    # 批次图方法，使GPU并行计算\n",
    "    batched_graph = dgl.batch(graphs)  # 对图数据进行按批次重组 !!!批次介绍！！\n",
    "    # 对其它数据进行张量转化\n",
    "    offsets_bert = torch.stack([torch.LongTensor(x) for x in bert_offsets], dim=0)\n",
    "    offsets_gcn = torch.stack([torch.LongTensor(x) for x in gcn_offsets], dim=0)\n",
    "    one_hot_labels = torch.from_numpy(np.asarray(labels)).type(torch.long)  # .squeeze()#必须要用long\n",
    "    bert_embeddings = torch.from_numpy(np.asarray(bert_embeddings))\n",
    "\n",
    "    return batched_graph, offsets_bert, offsets_gcn, bert_embeddings, one_hot_labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\latest\\lib\\site-packages\\dgl\\heterograph.py:93: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  \"Recommend creating graphs by `dgl.graph(data)`\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 将训练数据集转化为图数据\n",
    "all_graphs, gcn_offsets = getGraphsData(tokens_NoPUNC, offsets_NoPUNC, PROPN_bert, bert_forNoPUNC)\n",
    "train_y = getLabelData(df_train_val)  # 获取训练数据集的标签\n",
    "\n",
    "# 将测试数据集转化为图数据\n",
    "test_all_graphs, test_gcn_offsets = getGraphsData(test_tokens_NoPUNC, test_offsets_NoPUNC,\n",
    "                                                  test_PROPN_bert, test_bert_forNoPUNC)\n",
    "test_y = getLabelData(df_test)  # 获取测试数据集的标签\n",
    "# 生成测试数据集\n",
    "test_dataset = GPRDataset(test_y, test_all_graphs, test_offsets_NoPUNC, test_gcn_offsets, test_PROPN_bert)\n",
    "# 生成测试数据集的加载器\n",
    "test_dataloarder = DataLoader(test_dataset, collate_fn=collate, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "\n",
    "class RGCNModel(nn.Module):  # 多层R-GCN模型\n",
    "    def __init__(self, h_dim, num_rels, out_dim=256, num_hidden_layers=1):\n",
    "        super(RGCNModel, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()  # 定义网络层列表\n",
    "\n",
    "        for _ in range(num_hidden_layers):\n",
    "            rgcn_layer = RelGraphConv(h_dim, out_dim, num_rels, activation=F.relu)\n",
    "            self.layers.append(rgcn_layer)\n",
    "\n",
    "    def forward(self, g):\n",
    "        # 逐层处理\n",
    "        for layer in self.layers:\n",
    "            g.ndata['h'] = layer(g, g.ndata['h'].to(device), etypes=g.edata['rel_type'].to(device),\n",
    "                                 norm=g.edata['norm'].to(device))\n",
    "\n",
    "        rst_hidden = []\n",
    "        for sub_g in dgl.unbatch(g):  # 按批次解包\n",
    "            rst_hidden.append(sub_g.ndata['h'])\n",
    "        return rst_hidden\n",
    "\n",
    "\n",
    "# Design the Main Model (R-GCN + FFNN)\n",
    "class BERT_Head(nn.Module):\n",
    "    def __init__(self, bert_hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(bert_hidden_size * 3),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(bert_hidden_size * 3, 512 * 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        for i, module in enumerate(self.fc):\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if getattr(module, \"weight_v\", None) is not None:\n",
    "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
    "                    nn.init.kaiming_normal_(module.weight_v)\n",
    "                    assert model[i].weight_g is not None\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, bert_embeddings):\n",
    "        # print('BERT_Head bert_embeddings: ', bert_embeddings, bert_embeddings.view(bert_embeddings.shape[0],-1).shape)\n",
    "        outputs = self.fc(bert_embeddings.view(bert_embeddings.shape[0], -1))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"The MLP submodule\"\"\"\n",
    "\n",
    "    def __init__(self, gcn_out_size: int, bert_out_size: int):\n",
    "        super().__init__()\n",
    "        self.bert_out_size = bert_out_size\n",
    "        self.gcn_out_size = gcn_out_size\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(bert_out_size * 3 + gcn_out_size * 3),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(bert_out_size * 3 + gcn_out_size * 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 3),\n",
    "        )\n",
    "        for i, module in enumerate(self.fc):\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if getattr(module, \"weight_v\", None) is not None:\n",
    "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
    "                    nn.init.kaiming_normal_(module.weight_v)\n",
    "                    assert model[i].weight_g is not None\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, gcn_outputs, offsets_gcn, bert_embeddings):\n",
    "        '''\n",
    "        \n",
    "        Args:\n",
    "            gcn_outputs: \n",
    "            offsets_gcn:        用来从gcn_outputs中提取ABP的表示\n",
    "            bert_embeddings: \n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        gcn_extracted_outputs = [gcn_outputs[i].unsqueeze(0).gather(1, offsets_gcn[i].unsqueeze(0).unsqueeze(2)\n",
    "                                                                    .expand(-1, -1,\n",
    "                                                                            gcn_outputs[i].unsqueeze(0).size(2))).view(\n",
    "            gcn_outputs[i].unsqueeze(0).size(0), -1) for i in range(len(gcn_outputs))]\n",
    "\n",
    "        gcn_extracted_outputs = torch.stack(gcn_extracted_outputs, dim=0).squeeze()\n",
    "\n",
    "        embeddings = torch.cat((gcn_extracted_outputs, bert_embeddings), 1)\n",
    "\n",
    "        return self.fc(embeddings)\n",
    "\n",
    "\n",
    "class GPRModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.RGCN = RGCNModel(h_dim=768, out_dim=256, num_rels=3)\n",
    "        self.BERThead = BERT_Head(768)  # bert output size\n",
    "        self.head = Head(256, 512)  # gcn output   berthead output\n",
    "\n",
    "    # offsets_bert 未被使用\n",
    "    def forward(self, offsets_bert, offsets_gcn, bert_embeddings, g):\n",
    "        '''\n",
    "        \n",
    "        Args:\n",
    "            offsets_bert: not used\n",
    "            offsets_gcn: \n",
    "            bert_embeddings: \n",
    "            g: \n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        gcn_outputs = self.RGCN(g)\n",
    "        bert_head_outputs = self.BERThead(bert_embeddings)\n",
    "        head_outputs = self.head(gcn_outputs, offsets_gcn, bert_head_outputs)\n",
    "        return head_outputs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\n",
    "def adjust_learning_rate(optimizers, epoch, lr_value):\n",
    "    if epoch < 10:\n",
    "        # warm up\n",
    "        lr_tmp = 0.00001\n",
    "    else:\n",
    "        # 随着 epoch 的增加而降低学习率，0.9 是一个指数项，控制学习率下降的速度。\n",
    "        lr_tmp = lr_value * pow(1 - 1.0 * epoch / 100, 0.9)\n",
    "    if epoch > 36:\n",
    "        # 神奇，跟lr_value无关了\n",
    "        lr_tmp = 0.000015 * pow(1 - 1.0 * epoch / 100, 0.9)\n",
    "    for optimizer in optimizers:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_tmp\n",
    "    return lr_tmp\n",
    "\n",
    "\n",
    "def trainmodel(train_dataloarder, val_dataloarder, model, loss_func, optimizer, lr_value):\n",
    "    reg_lambda = 0.035\n",
    "    # total_epoch = 100\n",
    "    total_epoch = 10\n",
    "    best_val_loss = 11\n",
    "    # Cross-Entropy\n",
    "    ce_losses = []\n",
    "    epoch_losses = []\n",
    "    val_losses = []\n",
    "    val_acclist = []\n",
    "    for epoch in range(total_epoch):\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('|', \">\" * epoch, \" \" * (total_epoch - epoch), '|')\n",
    "\n",
    "        lr = adjust_learning_rate([optimizer], epoch, lr_value)\n",
    "        print(\"Learning rate = %4f\\n\" % lr)\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        reg_loss = 0\n",
    "        ce_loss = 0\n",
    "        # collate的返回值\n",
    "        for iter, (batched_graph, offsets_bert, offsets_gcn, bert_embeddings, labels) in enumerate(train_dataloarder):\n",
    "            # offsets_bert: torch.Size([4, 3]) ABP这3个单词的索引\n",
    "            # torch.Size([4, 1, 3, 768]) ABP这3个单词的BERT\n",
    "            bert_embeddings = bert_embeddings.to(device)\n",
    "            # torch.Size([4]) tensor([0, 0, 0, 2])\n",
    "            labels = labels.to(device)\n",
    "            # torch.Size([4, 3]) ABP在子图中的节点编号 tensor([[0, 2, 5], [0, 2, 5], [1, 4, 7], [1, 6, 8]])\n",
    "            offsets_gcn = offsets_gcn.to(device)\n",
    "            # batched_graph g.batch_size 4,g.batch_num_nodes [6, 6, 8, 6],g.batch_num_edges[12, 14, 20, 16]\n",
    "            # torch.Size([4, 3]) 分类为0 1 2 的得分值\n",
    "            prediction = model(offsets_bert, offsets_gcn, bert_embeddings, batched_graph)\n",
    "            \n",
    "            # 在损失函数中添加正则化项，惩罚模型的权重，从而防止过拟合。对权重向量中的每个元素的平方求和后取平方根。\n",
    "            # 有些文献和实验中使用平方 L2 正则化，有些则使用 L2 范数的平方根。两者之间的差异通常是微小的，而不取平方根可以简化计算。\n",
    "            # l2_reg = None\n",
    "            # for w in model.RGCN.parameters():\n",
    "            #     if not l2_reg:\n",
    "            #         l2_reg = w.norm(2)\n",
    "            #     else:\n",
    "            #         l2_reg = l2_reg + w.norm(2)\n",
    "            # for w in model.head.parameters():\n",
    "            #     if not l2_reg:\n",
    "            #         l2_reg = w.norm(2)\n",
    "            #     else:\n",
    "            #         l2_reg = l2_reg + w.norm(2)\n",
    "            # for w in model.BERThead.parameters():\n",
    "            #     if not l2_reg:\n",
    "            #         l2_reg = w.norm(2)\n",
    "            #     else:\n",
    "            #         l2_reg = l2_reg + w.norm(2)\n",
    "            l2_reg = sum(w.norm(2) for w in list(model.RGCN.parameters()) + list(model.head.parameters()) + list(model.BERThead.parameters()))\n",
    "            loss = loss_func(prediction, labels) + l2_reg * reg_lambda\n",
    "            # loss = loss_func(prediction, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ###########################\n",
    "            epoch_loss += loss.detach().item()\n",
    "            reg_loss += (l2_reg * reg_lambda).detach().item()\n",
    "            ce_loss += (loss_func(prediction, labels)).detach().item()\n",
    "        epoch_loss /= (iter + 1)\n",
    "        ce_loss /= (iter + 1)\n",
    "        reg_loss /= (iter + 1)\n",
    "        print('Epoch {}, loss {:.4f}, ce_loss {:.4f}, reg_loss {:.4f}'.format(epoch, epoch_loss, ce_loss, reg_loss))\n",
    "        print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        ce_losses.append(ce_loss)\n",
    "        ##################################\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        val_accs = []\n",
    "        for iter, (batched_graph, offsets_bert, offsets_gcn, bert_embeddings, labels) in enumerate(val_dataloarder):\n",
    "            offsets_gcn = offsets_gcn.to(device)\n",
    "            bert_embeddings = bert_embeddings.to(device)\n",
    "            labelsgpu = labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                prediction = model(offsets_bert, offsets_gcn, bert_embeddings, batched_graph)\n",
    "            loss = loss_func(prediction, labelsgpu)\n",
    "            val_loss += loss.detach().item()\n",
    "            # 调库计算正确率\n",
    "            val_acc = metrics.accuracy_score(labels, torch.argmax(prediction, -1).cpu().numpy())\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "        val_loss = val_loss / (iter + 1)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acclist.append(np.mean(val_accs))\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print('Epoch {}, val_loss {:.4f}, val_acc {:.4f}'.format(epoch,\n",
    "                                                                     val_loss, np.mean(val_accs)))\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if epoch > 20:\n",
    "                torch.save(model.state_dict(), base_dir + 'best_model.pth')\n",
    "            if epoch > 36: print('Best val loss found: ', best_val_loss)\n",
    "\n",
    "        ################\n",
    "        print('Epoch {}, val_loss {:.4f}, val_acc {:.4f}'.format(epoch,\n",
    "                                                                 val_loss, np.mean(val_accs)))\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "    #########################\n",
    "    print('This fold, the best val loss is: ', best_val_loss)\n",
    "    return ce_losses, val_losses, val_acclist\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 1\n",
      "====================\n",
      "Dataloader Success---------------------\n",
      "|                                                                                   |\n",
      "Learning rate = 0.000010\n",
      "\n",
      "Epoch 0, loss 10.3303, ce_loss 2.0626, reg_loss 8.2677\n",
      "Epoch 0, loss 10.3303\n",
      "Epoch 0, val_loss 1.3484, val_acc 0.4121\n",
      "Epoch 0, val_loss 1.3484, val_acc 0.4121\n",
      "This fold, the best val loss is:  1.348385441672142\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This fold, the test loss is:  tensor(0.5651)  acc is  0.7665\n",
      "====================\n",
      "Fold 2\n",
      "====================\n",
      "Dataloader Success---------------------\n",
      "|                                                                                   |\n",
      "Learning rate = 0.000010\n",
      "\n",
      "Epoch 0, loss 10.2131, ce_loss 1.9473, reg_loss 8.2658\n",
      "Epoch 0, loss 10.2131\n",
      "Epoch 0, val_loss 1.2069, val_acc 0.4023\n",
      "Epoch 0, val_loss 1.2069, val_acc 0.4023\n",
      "This fold, the best val loss is:  1.2068971662936847\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This fold, the test loss is:  tensor(0.5651)  acc is  0.7665\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5 fold\n",
    "\n",
    "# kfold = StratifiedKFold(n_splits=5)\n",
    "kfold = StratifiedKFold(n_splits=2)\n",
    "\n",
    "\n",
    "def getdataloader(index, isshuffle=False):\n",
    "    dataset = GPRDataset(train_y[index],\n",
    "                         list(itemgetter(*index)(all_graphs)),\n",
    "                         list(itemgetter(*index)(offsets_NoPUNC)),\n",
    "                         list(itemgetter(*index)(gcn_offsets)),\n",
    "                         list(itemgetter(*index)(PROPN_bert)))\n",
    "    dataloarder = DataLoader(dataset, collate_fn=collate,\n",
    "                             batch_size=4, shuffle=isshuffle)\n",
    "\n",
    "    return dataloarder\n",
    "\n",
    "\n",
    "\n",
    "test_predict_lst = []  # the test output for every fold\n",
    "for train_index, val_index in kfold.split(df_train_val, train_y):  # 循环5次\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Fold {len(test_predict_lst) + 1}\")\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    val_dataloarder = getdataloader(val_index)\n",
    "    train_dataloarder = getdataloader(train_index, True)\n",
    "    print('Dataloader Success---------------------')\n",
    "\n",
    "    model = GPRModel().to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    lr_value = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_value)\n",
    "    # ce: CrossEntropy 内含训练+验证、model.train()\n",
    "    ce_losses, val_losses, val_accs = trainmodel(train_dataloarder,\n",
    "                                                 val_dataloarder,\n",
    "                                                 model, loss_func, optimizer, lr_value)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ce_losses, label='CE_loss')\n",
    "    plt.plot(val_losses, label='Val_loss')\n",
    "    plt.plot(val_accs, label='Val_acc')\n",
    "    plt.legend()  # 添加图例\n",
    "    plt.show()\n",
    "\n",
    "    # 测试\n",
    "    test_loss = 0.\n",
    "    test_predict = None\n",
    "    model.load_state_dict(torch.load(base_dir + 'best_model.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for iter, (batched_graph, offsets_bert, offsets_gcn, bert_embeddings,\n",
    "               labels) in enumerate(test_dataloarder):\n",
    "\n",
    "        offsets_gcn = offsets_gcn.to(device)\n",
    "        bert_embeddings = bert_embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            prediction = model(offsets_bert, offsets_gcn, bert_embeddings, batched_graph)\n",
    "\n",
    "        if test_predict is None:\n",
    "            test_predict = prediction\n",
    "        else:\n",
    "            test_predict = torch.cat((test_predict, prediction), 0)\n",
    "        loss = loss_func(prediction, labels)\n",
    "        test_loss += loss\n",
    "\n",
    "    acc = metrics.accuracy_score(test_y, torch.argmax(test_predict, -1).cpu().numpy())\n",
    "    test_loss /= (iter + 1)\n",
    "    print('This fold, the test loss is: ', test_loss, \" acc is \", acc)\n",
    "    test_predict_lst.append(test_predict)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_predict_lst\n",
      "Type: <class 'list'>\n",
      "Shape: 5\n",
      "Index: 0\n",
      "  Type: <class 'torch.Tensor'>\n",
      "  Shape: torch.Size([2000, 3])\n",
      "  Data Type: torch.float32\n",
      "  Index: 0\n",
      "    Type: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([3])\n",
      "    Data Type: torch.float32\n",
      "    Index: 0\n",
      "      Type: <class 'torch.Tensor'>\n",
      "      Shape: torch.Size([])\n",
      "      Data Type: torch.float32\n",
      "tensor([1.4402, 0.3232, 0.1801])\n",
      "train_y\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (2454,)\n",
      "Data Type: int64\n",
      "Index: 0\n",
      "  Type: <class 'numpy.int64'>\n",
      "[1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "## Test\n",
    "print('test_predict_lst')\n",
    "# (5,2000,3) 3独热的分类得分，需要softmax\n",
    "print_variable_structure(test_predict_lst)\n",
    "print(test_predict_lst[0][0])\n",
    "print('train_y')\n",
    "# (2454,)\n",
    "print_variable_structure(train_y)\n",
    "print(train_y[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# Test Part\n",
    "test_predict_arr = [torch.softmax(pre.cpu(), -1).clamp(1e-4, 1 - 1e-4).numpy() for pre in test_predict_lst]\n",
    "# 求k fold得到的 k 个概率的均值\n",
    "final_test_preds = np.mean(test_predict_arr, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_predict_arr\n",
      "Type: <class 'list'>\n",
      "Shape: 2\n",
      "Index: 0\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (2000, 3)\n",
      "  Data Type: float32\n",
      "  Index: 0\n",
      "    Type: <class 'numpy.ndarray'>\n",
      "    Shape: (3,)\n",
      "    Data Type: float32\n",
      "    Index: 0\n",
      "      Type: <class 'numpy.float32'>\n",
      "[0.6207685  0.20315523 0.1760763 ]\n",
      "[0.6207685  0.20315523 0.1760763 ]\n",
      "final_test_preds\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (2000, 3)\n",
      "Data Type: float32\n",
      "Index: 0\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (3,)\n",
      "  Data Type: float32\n",
      "  Index: 0\n",
      "    Type: <class 'numpy.float32'>\n",
      "[0.6207685  0.20315523 0.1760763 ]\n"
     ]
    }
   ],
   "source": [
    "print('test_predict_arr')\n",
    "# (k, 2000, 3)\n",
    "print_variable_structure(test_predict_arr)\n",
    "# 不同折的测试结果竟然相等!\n",
    "# [0.6207685  0.20315523 0.1760763 ]\n",
    "print(test_predict_arr[0][0])\n",
    "print(test_predict_arr[1][0])\n",
    "\n",
    "print('final_test_preds')\n",
    "# (2000, 3)\n",
    "print_variable_structure(final_test_preds)\n",
    "# [0.6207685  0.20315523 0.1760763 ]\n",
    "print(final_test_preds[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "\n",
    "def extract_target(df):\n",
    "    df[\"Neither\"] = 0\n",
    "    df.loc[~(df['A-coref'] | df['B-coref']), \"Neither\"] = 1\n",
    "\n",
    "    df[\"target\"] = 0\n",
    "    df.loc[df['B-coref'] == 1, \"target\"] = 1\n",
    "    df.loc[df[\"Neither\"] == 1, \"target\"] = 2\n",
    "    return df\n",
    "\n",
    "# target 是正确结果，取值为 0 1 2\n",
    "test_df = extract_target(df_test)\n",
    "log_loss(test_df.target, final_test_preds)\n",
    "\n",
    "# result 是预测结果，取值为 0 1 2\n",
    "result = np.argmax(final_test_preds, -1).reshape(len(final_test_preds), 1)\n",
    "\n",
    "# 保存结果\n",
    "df_sub = pd.DataFrame(np.concatenate([final_test_preds, result], -1), columns=[\"A\", \"B\", \"NEITHER\", 'result'])\n",
    "df_sub[\"ID\"] = test_df.ID\n",
    "df_sub[\"target\"] = test_df[\"target\"]\n",
    "df_sub = df_sub[['ID', \"A\", \"B\", \"NEITHER\", \"result\", \"target\"]]\n",
    "df_sub.to_csv(base_dir + \"submission_415_copy3.csv\", index=False)\n",
    "\n",
    "acc = metrics.accuracy_score(test_df[\"target\"].values, np.argmax(final_test_preds, -1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.target\n",
      "Type: <class 'pandas.core.series.Series'>\n",
      "Attributes: ['_is_copy', '_mgr', '_item_cache', '_attrs', '_flags', '_name', '_index', '_cacher']\n",
      "Attribute: _is_copy\n",
      "  Type: <class 'NoneType'>\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: target, dtype: int64\n",
      "result\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (2000, 1)\n",
      "Data Type: int64\n",
      "Index: 0\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (1,)\n",
      "  Data Type: int64\n",
      "  Index: 0\n",
      "    Type: <class 'numpy.int64'>\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "df_sub\n",
      "                ID         A         B   NEITHER  result  target\n",
      "0    development-1  0.620768  0.203155  0.176076     0.0       0\n",
      "1    development-2  0.949732  0.014193  0.036075     0.0       0\n",
      "2    development-3  0.020058  0.951935  0.028007     1.0       1\n",
      "3    development-4  0.076145  0.472655  0.451201     1.0       1\n",
      "4    development-5  0.076833  0.898564  0.024602     1.0       1\n",
      "5    development-6  0.835568  0.151861  0.012571     0.0       0\n",
      "6    development-7  0.798665  0.152538  0.048796     0.0       2\n",
      "7    development-8  0.119522  0.740976  0.139502     1.0       1\n",
      "8    development-9  0.028281  0.946336  0.025383     1.0       1\n",
      "9   development-10  0.666055  0.299076  0.034869     0.0       0\n",
      "10  development-11  0.073760  0.710725  0.215515     1.0       1\n",
      "11  development-12  0.897549  0.016433  0.086018     0.0       0\n",
      "12  development-13  0.613438  0.361748  0.024814     0.0       0\n",
      "13  development-14  0.694332  0.178089  0.127579     0.0       0\n",
      "14  development-15  0.668445  0.297337  0.034218     0.0       0\n",
      "15  development-16  0.202545  0.614353  0.183102     1.0       2\n",
      "16  development-17  0.235829  0.626497  0.137674     1.0       0\n",
      "17  development-18  0.169369  0.073705  0.756927     2.0       2\n",
      "18  development-19  0.127449  0.620946  0.251604     1.0       1\n",
      "19  development-20  0.106887  0.553194  0.339919     1.0       1\n",
      "acc 0.7665\n"
     ]
    }
   ],
   "source": [
    "## Test\n",
    "print('test_df.target')\n",
    "print_variable_structure(test_df.target)\n",
    "print(test_df.target[:5])\n",
    "\n",
    "print('result')\n",
    "print_variable_structure(result)\n",
    "print(result[:5])\n",
    "\n",
    "print('df_sub')\n",
    "print(df_sub.head(20))\n",
    "print('acc', acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
